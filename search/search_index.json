{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RiteLLM","text":""},{"location":"#overview","title":"Overview","text":"<p>RiteLLM is a high-performance LLM (Large Language Model) gateway that provides a unified interface for interacting with multiple LLM providers. Built with Rust and exposed through elegant Python bindings, RiteLLM combines the speed of compiled systems programming with the ease of Python development.</p>"},{"location":"#key-features","title":"\u2728 Key Features","text":"<ul> <li>\ud83d\ude80 Unified LLM Gateway: Single, consistent API for multiple LLM providers</li> <li>\ud83d\udd0c Provider Support: Currently supports OpenAI, with more providers coming soon (Anthropic, Google, Cohere, and more)</li> <li>\u26a1 Rust-Powered Performance: Core engine built in Rust for maximum speed and efficiency</li> <li>\ud83d\udcca First-Class Observability: Built-in integration with Weights &amp; Biases Weave for seamless tracing, monitoring, and debugging</li> <li>\ud83d\udc0d Pythonic Interface: Clean, intuitive Python API that feels native to the ecosystem</li> <li>\ud83d\udd12 Type-Safe: Full type hints for better IDE support and code quality</li> <li>\ud83c\udf0a Streaming Support: Real-time streaming responses for better user experience</li> </ul>"},{"location":"#installation","title":"\ud83d\ude80 Installation","text":"<p>Install RiteLLM using pip:</p> <pre><code>uv pip install ritellm\n</code></pre>"},{"location":"#quick-start","title":"\ud83d\udcbb Quick Start","text":""},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from ritellm import completion\n\n# Define your messages\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain what Rust is in one sentence.\"}\n]\n\n# Make a completion request\nresponse = completion(\n    model=\"openai/gpt-3.5-turbo\",\n    messages=messages,\n    temperature=0.7,\n    max_tokens=100\n)\n\n# Access the response\nprint(response[\"choices\"][0][\"message\"][\"content\"])\nprint(f\"Tokens used: {response['usage']['total_tokens']}\")\n</code></pre>"},{"location":"#streaming-responses","title":"Streaming Responses","text":"<p>For real-time streaming of responses as they are generated:</p> <pre><code>from ritellm import completion\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Write a short poem about Rust.\"}\n]\n\n# Enable streaming\nresponse = completion(\n    model=\"openai/gpt-3.5-turbo\",\n    messages=messages,\n    stream=True  # Enable streaming\n)\n\n# Stream the response\nfor chunk in response:\n    if \"choices\" in chunk and len(chunk[\"choices\"]) &gt; 0:\n        content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n        if content:\n            print(content, end=\"\", flush=True)\n\nprint()  # New line after streaming completes\n</code></pre> <p>See the Streaming Guide for more details.</p>"},{"location":"#async-usage","title":"Async Usage","text":"<p>For concurrent requests and non-blocking API calls, use the async <code>acompletion</code> function:</p> <pre><code>import asyncio\nfrom ritellm import acompletion\n\nasync def main():\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n\n    # Non-blocking async call\n    response = await acompletion(\n        model=\"openai/gpt-3.5-turbo\",\n        messages=messages\n    )\n\n    print(response[\"choices\"][0][\"message\"][\"content\"])\n\nasyncio.run(main())\n</code></pre> <p>See the Async Usage Guide for more details on async mode and concurrent requests.</p>"},{"location":"#with-weave-tracing","title":"With Weave Tracing","text":"<p>RiteLLM has first-class support for Weave, enabling automatic tracing and monitoring of your LLM calls:</p> <pre><code>import weave\nfrom ritellm import completion\n\n# Initialize Weave\nweave.init(project_name=\"my-llm-project\")\n\n# Wrap completion with Weave's op decorator for automatic tracing\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n]\n\nresponse = weave.op(completion)(\n    model=\"openai/gpt-3.5-turbo\",\n    messages=messages,\n    temperature=0.7\n)\n\n# Your calls are now automatically traced in Weave!\n</code></pre>"},{"location":"#gratitude","title":"\ud83d\ude4f Gratitude","text":"<ul> <li><code>ritellm</code> is highly inspired by litellm and its simple API design.</li> <li>Made with \u2764\ufe0f and \ud83e\udd80</li> </ul>"},{"location":"api-reference/python/completion/","title":"Python","text":""},{"location":"api-reference/python/completion/#ritellm.completion","title":"<code>ritellm.completion(model, messages, temperature=None, max_tokens=None, base_url=None, stream=False, additional_params=None)</code>","text":"<p>Clean Python wrapper around the completion_gateway function.</p> <p>This function provides a convenient interface to call various LLM providers' chat completion APIs through the Rust-backed completion_gateway binding. The model string should include a provider prefix.</p> <p>Example</p> <pre><code>from ritellm import completion\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n]\n\n# Non-streaming\nresponse = completion(model=\"openai/gpt-3.5-turbo\", messages=messages)\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n\n# Streaming\nresponse = completion(model=\"openai/gpt-3.5-turbo\", messages=messages, stream=True)\nfor chunk in response:\n    print(chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\")\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model to use with provider prefix (e.g., \"openai/gpt-4\", \"openai/gpt-3.5-turbo\")</p> required <code>messages</code> <code>list</code> <p>A list of message dictionaries with \"role\" and \"content\" keys</p> required <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0 to 2.0)</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens to generate</p> <code>None</code> <code>base_url</code> <code>str</code> <p>Base URL for the API endpoint</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Enable streaming responses (default: False)</p> <code>False</code> <code>additional_params</code> <code>str</code> <p>Additional parameters as a JSON string</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any] | Iterator[dict[str, Any]]</code> <p>dict | Iterator[dict]: A dictionary containing the API response, or an iterator of chunks if stream=True</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provider prefix is not supported</p> Environment Variables <p>OPENAI_API_KEY: Required for OpenAI models</p>"},{"location":"api-reference/python/completion/#ritellm.acompletion","title":"<code>ritellm.acompletion(model, messages, temperature=None, max_tokens=None, base_url=None, stream=False, additional_params=None)</code>  <code>async</code>","text":"<p>Async Python wrapper around the async_completion_gateway function.</p> <p>This function provides an async interface to call various LLM providers' chat completion APIs through the Rust-backed async_completion_gateway binding. The model string should include a provider prefix.</p> <p>Example</p> <pre><code>import asyncio\nfrom ritellm import acompletion\n\nasync def main():\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n\n    # Non-streaming\n    response = await acompletion(model=\"openai/gpt-3.5-turbo\", messages=messages)\n    print(response[\"choices\"][0][\"message\"][\"content\"])\n\n    # Streaming\n    response = await acompletion(model=\"openai/gpt-3.5-turbo\", messages=messages, stream=True)\n    for chunk in response:\n        print(chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\"), end=\"\")\n\nasyncio.run(main())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>The model to use with provider prefix (e.g., \"openai/gpt-4\", \"openai/gpt-3.5-turbo\")</p> required <code>messages</code> <code>list</code> <p>A list of message dictionaries with \"role\" and \"content\" keys</p> required <code>temperature</code> <code>float</code> <p>Sampling temperature (0.0 to 2.0)</p> <code>None</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens to generate</p> <code>None</code> <code>base_url</code> <code>str</code> <p>Base URL for the API endpoint</p> <code>None</code> <code>stream</code> <code>bool</code> <p>Enable streaming responses (default: False)</p> <code>False</code> <code>additional_params</code> <code>str</code> <p>Additional parameters as a JSON string</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Any] | Iterator[dict[str, Any]]</code> <p>dict | Iterator[dict]: A dictionary containing the API response, or an iterator of chunks if stream=True</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provider prefix is not supported</p> Environment Variables <p>OPENAI_API_KEY: Required for OpenAI models</p>"},{"location":"guides/async-usage/","title":"Async Usage","text":"<p>RiteLLM provides async support through the <code>acompletion</code> function, which allows you to make non-blocking API calls and handle concurrent requests efficiently.</p>"},{"location":"guides/async-usage/#basic-async-usage","title":"Basic Async Usage","text":"<p>The simplest way to use async mode is with the <code>acompletion</code> function:</p> <pre><code>import asyncio\nfrom ritellm import acompletion\n\nasync def main():\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"}\n    ]\n\n    response = await acompletion(\n        model=\"openai/gpt-3.5-turbo\",\n        messages=messages\n    )\n\n    print(response[\"choices\"][0][\"message\"][\"content\"])\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/async-usage/#async-with-streaming","title":"Async with Streaming","text":"<p>Enable streaming in async mode by setting <code>stream=True</code>:</p> <pre><code>import asyncio\nfrom ritellm import acompletion\n\nasync def main():\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Write a short poem about Python.\"}\n    ]\n\n    response = await acompletion(\n        model=\"openai/gpt-3.5-turbo\",\n        messages=messages,\n        stream=True\n    )\n\n    # Stream the response as it arrives\n    for chunk in response:\n        if \"choices\" in chunk and len(chunk[\"choices\"]) &gt; 0:\n            delta = chunk[\"choices\"][0].get(\"delta\", {})\n            content = delta.get(\"content\", \"\")\n            if content:\n                print(content, end=\"\", flush=True)\n\n    print()  # New line after streaming completes\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/async-usage/#concurrent-requests","title":"Concurrent Requests","text":"<p>One of the main benefits of async is handling multiple requests concurrently:</p> <pre><code>import asyncio\nfrom ritellm import acompletion\n\nasync def ask_question(question: str):\n    \"\"\"Ask a single question and return the response.\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": question}\n    ]\n\n    response = await acompletion(\n        model=\"openai/gpt-3.5-turbo\",\n        messages=messages,\n        max_tokens=100\n    )\n\n    return response[\"choices\"][0][\"message\"][\"content\"]\n\nasync def main():\n    # Define multiple questions\n    questions = [\n        \"What is Python?\",\n        \"What is Rust?\",\n        \"What is async programming?\"\n    ]\n\n    # Run all questions concurrently\n    tasks = [ask_question(q) for q in questions]\n    answers = await asyncio.gather(*tasks)\n\n    # Print all answers\n    for question, answer in zip(questions, answers):\n        print(f\"Q: {question}\")\n        print(f\"A: {answer}\\n\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/async-usage/#complete-example-async-chat-application","title":"Complete Example: Async Chat Application","text":"<p>Here's a complete example showing a simple async chat application:</p> <pre><code>import asyncio\nfrom ritellm import acompletion\n\nasync def chat_streaming(messages: list[dict], model: str = \"openai/gpt-3.5-turbo\"):\n    \"\"\"Send a chat message and stream the response.\"\"\"\n    response = await acompletion(\n        model=model,\n        messages=messages,\n        stream=True,\n        temperature=0.7,\n        max_tokens=500\n    )\n\n    print(\"Assistant: \", end=\"\", flush=True)\n    full_response = \"\"\n\n    for chunk in response:\n        if \"choices\" not in chunk or len(chunk[\"choices\"]) == 0:\n            continue\n\n        choice = chunk[\"choices\"][0]\n        delta = choice.get(\"delta\", {})\n        content = delta.get(\"content\", \"\")\n\n        if content:\n            print(content, end=\"\", flush=True)\n            full_response += content\n\n        if choice.get(\"finish_reason\") == \"stop\":\n            break\n\n    print(\"\\n\")\n    return full_response\n\nasync def main():\n    \"\"\"Simple chat loop with async streaming.\"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n    ]\n\n    # Simulate a conversation\n    user_messages = [\n        \"Hello! What can you help me with?\",\n        \"Tell me about async programming in Python.\",\n        \"Thanks!\"\n    ]\n\n    for user_msg in user_messages:\n        print(f\"User: {user_msg}\\n\")\n        messages.append({\"role\": \"user\", \"content\": user_msg})\n\n        # Get streaming response\n        assistant_msg = await chat_streaming(messages)\n        messages.append({\"role\": \"assistant\", \"content\": assistant_msg})\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/async-usage/#best-practices","title":"Best Practices","text":""},{"location":"guides/async-usage/#1-use-asynciogather-for-concurrent-requests","title":"1. Use <code>asyncio.gather()</code> for Concurrent Requests","text":"<p>When you need to make multiple API calls, use <code>asyncio.gather()</code> to run them concurrently:</p> <pre><code># Good: Concurrent requests\nresults = await asyncio.gather(\n    acompletion(model=\"openai/gpt-3.5-turbo\", messages=messages1),\n    acompletion(model=\"openai/gpt-3.5-turbo\", messages=messages2),\n    acompletion(model=\"openai/gpt-3.5-turbo\", messages=messages3)\n)\n\n# Bad: Sequential requests (slower)\nresult1 = await acompletion(model=\"openai/gpt-3.5-turbo\", messages=messages1)\nresult2 = await acompletion(model=\"openai/gpt-3.5-turbo\", messages=messages2)\nresult3 = await acompletion(model=\"openai/gpt-3.5-turbo\", messages=messages3)\n</code></pre>"},{"location":"guides/async-usage/#2-handle-errors-gracefully","title":"2. Handle Errors Gracefully","text":"<p>Wrap async calls in try-except blocks to handle failures:</p> <pre><code>async def safe_completion(messages):\n    try:\n        response = await acompletion(\n            model=\"openai/gpt-3.5-turbo\",\n            messages=messages\n        )\n        return response\n    except Exception as e:\n        print(f\"Error during completion: {e}\")\n        return None\n</code></pre>"},{"location":"guides/async-usage/#3-use-streaming-for-long-responses","title":"3. Use Streaming for Long Responses","text":"<p>For better user experience with long responses, use streaming:</p> <pre><code># Good for long responses: User sees content as it arrives\nresponse = await acompletion(\n    model=\"openai/gpt-3.5-turbo\",\n    messages=messages,\n    stream=True\n)\n\n# Less ideal for long responses: User waits for entire response\nresponse = await acompletion(\n    model=\"openai/gpt-3.5-turbo\",\n    messages=messages,\n    stream=False\n)\n</code></pre>"},{"location":"guides/async-usage/#4-rate-limiting-with-semaphores","title":"4. Rate Limiting with Semaphores","text":"<p>Control concurrency to avoid rate limits:</p> <pre><code>async def main():\n    # Limit to 5 concurrent requests\n    semaphore = asyncio.Semaphore(5)\n\n    async def limited_completion(messages):\n        async with semaphore:\n            return await acompletion(\n                model=\"openai/gpt-3.5-turbo\",\n                messages=messages\n            )\n\n    # Create many tasks but only 5 run at once\n    tasks = [limited_completion(msg) for msg in message_list]\n    results = await asyncio.gather(*tasks)\n</code></pre>"},{"location":"guides/async-usage/#comparison-sync-vs-async","title":"Comparison: Sync vs Async","text":""},{"location":"guides/async-usage/#synchronous-blocking","title":"Synchronous (Blocking)","text":"<pre><code>from ritellm import completion\n\n# Sequential execution - slow for multiple requests\nfor i in range(10):\n    response = completion(\n        model=\"openai/gpt-3.5-turbo\",\n        messages=[{\"role\": \"user\", \"content\": f\"Question {i}\"}]\n    )\n    print(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"guides/async-usage/#asynchronous-non-blocking","title":"Asynchronous (Non-blocking)","text":"<pre><code>import asyncio\nfrom ritellm import acompletion\n\nasync def main():\n    # Concurrent execution - fast!\n    tasks = [\n        acompletion(\n            model=\"openai/gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": f\"Question {i}\"}]\n        )\n        for i in range(10)\n    ]\n\n    results = await asyncio.gather(*tasks)\n    for result in results:\n        print(result[\"choices\"][0][\"message\"][\"content\"])\n\nasyncio.run(main())\n</code></pre>"},{"location":"guides/quickstart/","title":"Quickstart","text":""},{"location":"guides/quickstart/#basic-usage","title":"Basic Usage","text":"<pre><code>from ritellm import completion\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Hello!\"}\n]\n\nresponse = completion(\n    model=\"openai/gpt-3.5-turbo\",\n    messages=messages\n)\n\nprint(response[\"choices\"][0][\"message\"][\"content\"])\n</code></pre>"},{"location":"guides/quickstart/#streaming","title":"Streaming","text":"<p>To enable streaming, set <code>stream=True</code>:</p> <pre><code>from ritellm import completion\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Write a short poem.\"}\n]\n\nresponse = completion(\n    model=\"openai/gpt-3.5-turbo\",\n    messages=messages,\n    stream=True  # Enable streaming\n)\n\n# Iterate over chunks as they arrive\nfor chunk in response:\n    if \"choices\" in chunk and len(chunk[\"choices\"]) &gt; 0:\n        delta = chunk[\"choices\"][0].get(\"delta\", {})\n        content = delta.get(\"content\", \"\")\n        if content:\n            print(content, end=\"\", flush=True)\n\nprint()  # New line after streaming completes\n</code></pre>"},{"location":"guides/quickstart/#response-format","title":"Response Format","text":""},{"location":"guides/quickstart/#non-streaming-response","title":"Non-Streaming Response","text":"<p>When <code>stream=False</code> (default), you receive a complete response dictionary:</p> <pre><code>{\n    \"id\": \"chatcmpl-...\",\n    \"object\": \"chat.completion\",\n    \"created\": 1234567890,\n    \"model\": \"gpt-3.5-turbo-0125\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"message\": {\n                \"role\": \"assistant\",\n                \"content\": \"Hello! How can I help you today?\"\n            },\n            \"finish_reason\": \"stop\"\n        }\n    ],\n    \"usage\": {\n        \"prompt_tokens\": 20,\n        \"completion_tokens\": 10,\n        \"total_tokens\": 30\n    }\n}\n</code></pre>"},{"location":"guides/quickstart/#streaming-response","title":"Streaming Response","text":"<p>When <code>stream=True</code>, you receive an iterator of chunk dictionaries:</p> <pre><code># First chunk (usually empty or with role)\n{\n    \"id\": \"chatcmpl-...\",\n    \"object\": \"chat.completion.chunk\",\n    \"created\": 1234567890,\n    \"model\": \"gpt-3.5-turbo-0125\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"delta\": {\n                \"role\": \"assistant\",\n                \"content\": \"\"\n            },\n            \"finish_reason\": None\n        }\n    ]\n}\n\n# Content chunks\n{\n    \"id\": \"chatcmpl-...\",\n    \"object\": \"chat.completion.chunk\",\n    \"created\": 1234567890,\n    \"model\": \"gpt-3.5-turbo-0125\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"delta\": {\n                \"content\": \"Hello\"\n            },\n            \"finish_reason\": None\n        }\n    ]\n}\n\n# Final chunk\n{\n    \"id\": \"chatcmpl-...\",\n    \"object\": \"chat.completion.chunk\",\n    \"created\": 1234567890,\n    \"model\": \"gpt-3.5-turbo-0125\",\n    \"choices\": [\n        {\n            \"index\": 0,\n            \"delta\": {},\n            \"finish_reason\": \"stop\"\n        }\n    ]\n}\n</code></pre>"},{"location":"guides/quickstart/#complete-example","title":"Complete Example","text":"<p>Here's a complete example that handles streaming responses gracefully:</p> <pre><code>from ritellm import completion\n\ndef stream_completion(messages, model=\"openai/gpt-3.5-turbo\"):\n    \"\"\"Stream a completion and print the response.\"\"\"\n    response = completion(\n        model=model,\n        messages=messages,\n        stream=True,\n        temperature=0.7,\n        max_tokens=500\n    )\n\n    print(\"Assistant: \", end=\"\", flush=True)\n    full_response = \"\"\n\n    for chunk in response:\n        if \"choices\" not in chunk or len(chunk[\"choices\"]) == 0:\n            continue\n\n        choice = chunk[\"choices\"][0]\n        delta = choice.get(\"delta\", {})\n        content = delta.get(\"content\", \"\")\n\n        if content:\n            print(content, end=\"\", flush=True)\n            full_response += content\n\n        # Check if streaming is complete\n        if choice.get(\"finish_reason\") == \"stop\":\n            break\n\n    print()  # New line\n    return full_response\n\n\n# Usage\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n]\n\nresponse_text = stream_completion(messages)\nprint(f\"\\n\\nFull response length: {len(response_text)} characters\")\n</code></pre>"},{"location":"guides/quickstart/#best-practices","title":"Best Practices","text":"<ol> <li>Always handle missing content: Not all chunks will have content, especially the first and last chunks.</li> </ol> <pre><code>content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\nif content:\n    print(content, end=\"\", flush=True)\n</code></pre> <ol> <li> <p>Use flush=True: When printing streaming content, use <code>flush=True</code> to ensure immediate output.</p> </li> <li> <p>Check finish_reason: Monitor the <code>finish_reason</code> field to know when streaming is complete.</p> </li> <li> <p>Error handling: Wrap streaming in try-except blocks to handle network issues gracefully.</p> </li> </ol> <pre><code>try:\n    for chunk in response:\n        # Process chunk\n        pass\nexcept Exception as e:\n    print(f\"\\nStreaming error: {e}\")\n</code></pre> <ol> <li>Accumulate response: If you need the full response text, accumulate it from the chunks:</li> </ol> <pre><code>full_text = \"\"\nfor chunk in response:\n    content = chunk[\"choices\"][0][\"delta\"].get(\"content\", \"\")\n    full_text += content\n</code></pre>"},{"location":"guides/quickstart/#supported-providers","title":"Supported Providers","text":"<p>Currently, streaming is supported for:</p> <ul> <li>\u2705 OpenAI (<code>openai/</code> prefix)</li> </ul> <p>More providers will be added in future releases.</p>"},{"location":"guides/rust-usage/","title":"Using RiteLLM from Rust","text":""},{"location":"guides/rust-usage/#important-note","title":"Important Note","text":"<p>RiteLLM is designed as a Python library with Rust internals. The <code>completion_gateway</code> function and related code are not directly callable from pure Rust code. They are Python functions (using PyO3) meant to be called from Python.</p>"},{"location":"guides/rust-usage/#architecture-overview","title":"Architecture Overview","text":"<p>The library uses the following architecture:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Python User Code                \u2502\n\u2502  (imports ritellm, calls completion())  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       PyO3 Python Bindings              \u2502\n\u2502  (completion_gateway in src/lib.rs)     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                 \u2502\n                 \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502       Rust Implementation               \u2502\n\u2502  (openai_completion_async in openai.rs) \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"guides/rust-usage/#why-cant-you-call-it-directly-from-rust","title":"Why Can't You Call It Directly from Rust?","text":"<p>The library is built as a Python extension module (<code>cdylib</code> crate type) with PyO3's <code>extension-module</code> feature. This means:</p> <ol> <li> <p>Python symbols are expected at runtime: The library expects to be loaded by a Python interpreter which provides symbols like <code>PyObject_CallNoArgs</code>, <code>PyUnicode_FromStringAndSize</code>, etc.</p> </li> <li> <p>Not a Rust library: It's compiled as a shared library (<code>.so</code>/<code>.dylib</code>/<code>.dll</code>) for Python to load, not as a Rust library (<code>.rlib</code>) that other Rust code can link against.</p> </li> <li> <p>PyO3 types everywhere: The functions use PyO3-specific types (<code>PyResult</code>, <code>Python</code>, <code>PyAny</code>) that only make sense in the context of Python integration.</p> </li> </ol>"},{"location":"guides/rust-usage/#understanding-the-code","title":"Understanding the Code","text":"<p>Here's what the <code>completion_gateway</code> function does (from <code>src/lib.rs</code>):</p> <pre><code>#[pyfunction]  // This makes it callable from Python only\n#[pyo3(signature = (model, messages, temperature=None, ...))]\nfn completion_gateway(\n    py: Python,  // Python GIL guard - only available in Python context\n    model: String,\n    messages: Vec&lt;HashMap&lt;String, String&gt;&gt;,\n    // ... more parameters\n) -&gt; PyResult&lt;Py&lt;PyAny&gt;&gt; {  // Returns Python object\n    if model.starts_with(\"openai/\") {\n        let actual_model = model.strip_prefix(\"openai/\").unwrap().to_string();\n        openai_completion(py, actual_model, messages, ...)\n    } else {\n        Err(PyErr::new::&lt;PyValueError, _&gt;(...))\n    }\n}\n</code></pre> <p>The key indicators that this is Python-only: - <code>#[pyfunction]</code> macro - marks it as a Python function - <code>py: Python</code> parameter - a handle to the Python interpreter - <code>PyResult</code> return type - wraps errors for Python - <code>Py&lt;PyAny&gt;</code> - a Python object reference</p>"},{"location":"guides/rust-usage/#how-to-use-ritellm-from-rust","title":"How to Use RiteLLM from Rust","text":"<p>You have two options:</p>"},{"location":"guides/rust-usage/#option-1-use-the-python-api-from-rust-recommended","title":"Option 1: Use the Python API from Rust (Recommended)","text":"<p>If you want to use RiteLLM from a Rust application, call the Python API using a Python embedding library:</p> <pre><code>[dependencies]\npyo3 = \"0.27\"\n</code></pre> <pre><code>use pyo3::prelude::*;\nuse pyo3::types::{PyDict, PyList};\n\nfn main() -&gt; PyResult&lt;()&gt; {\n    Python::with_gil(|py| {\n        // Import the ritellm module\n        let ritellm = py.import(\"ritellm\")?;\n\n        // Create messages\n        let messages = PyList::new(py, vec![\n            {\n                let msg = PyDict::new(py);\n                msg.set_item(\"role\", \"user\")?;\n                msg.set_item(\"content\", \"Hello from Rust!\")?;\n                msg\n            }\n        ])?;\n\n        // Call completion\n        let response = ritellm.call_method1(\n            \"completion\",\n            (\n                \"openai/gpt-3.5-turbo\",\n                messages,\n            )\n        )?;\n\n        // Process response\n        println!(\"Response: {:?}\", response);\n        Ok(())\n    })\n}\n</code></pre>"},{"location":"guides/rust-usage/#option-2-reimplement-the-logic-in-pure-rust","title":"Option 2: Reimplement the Logic in Pure Rust","text":"<p>If you need a pure Rust solution without Python, you would need to reimplement the logic using the same dependencies:</p> <pre><code>[dependencies]\nreqwest = { version = \"0.12\", features = [\"json\", \"rustls-tls\", \"stream\"] }\nserde_json = \"1.0\"\ntokio = { version = \"1.40\", features = [\"full\"] }\n</code></pre> <pre><code>use serde_json::json;\nuse std::env;\n\n#[tokio::main]\nasync fn main() -&gt; Result&lt;(), Box&lt;dyn std::error::Error&gt;&gt; {\n    let api_key = env::var(\"OPENAI_API_KEY\")?;\n    let client = reqwest::Client::new();\n\n    let body = json!({\n        \"model\": \"gpt-3.5-turbo\",\n        \"messages\": [\n            {\"role\": \"user\", \"content\": \"Hello from pure Rust!\"}\n        ],\n        \"temperature\": 0.7,\n        \"max_tokens\": 100\n    });\n\n    let response = client\n        .post(\"https://api.openai.com/v1/chat/completions\")\n        .header(\"Authorization\", format!(\"Bearer {}\", api_key))\n        .header(\"Content-Type\", \"application/json\")\n        .json(&amp;body)\n        .send()\n        .await?;\n\n    let response_text = response.text().await?;\n    println!(\"Response: {}\", response_text);\n\n    Ok(())\n}\n</code></pre> <p>This approach gives you full control but loses the benefits of RiteLLM's abstraction layer.</p>"},{"location":"guides/rust-usage/#viewing-the-rust-implementation","title":"Viewing the Rust Implementation","text":"<p>You can view the Rust implementation to understand how it works:</p> <ul> <li>Gateway logic: <code>src/lib.rs</code> - Routes requests to appropriate providers</li> <li>OpenAI implementation: <code>src/openai.rs</code> - Handles OpenAI API calls</li> <li><code>openai_completion()</code> - Python-facing function</li> <li><code>openai_completion_async()</code> - Async implementation</li> </ul> <p>The core async function (<code>openai_completion_async</code>) contains the actual HTTP logic: <pre><code>pub async fn openai_completion_async(\n    api_key: String,\n    model: String,\n    messages: Vec&lt;HashMap&lt;String, String&gt;&gt;,\n    temperature: Option&lt;f32&gt;,\n    max_tokens: Option&lt;i32&gt;,\n    base_url: Option&lt;String&gt;,\n    stream: Option&lt;bool&gt;,\n    additional_params: Option&lt;String&gt;,\n) -&gt; PyResult&lt;CompletionResult&gt;\n</code></pre></p> <p>However, even this function returns <code>PyResult</code>, so it's still tied to Python.</p>"},{"location":"guides/rust-usage/#future-plans","title":"Future Plans","text":"<p>To make RiteLLM usable from pure Rust, the project would need:</p> <ol> <li>Separate the core logic into a pure Rust crate (e.g., <code>ritellm-core</code>)</li> <li>Create Python bindings in a separate crate (e.g., <code>ritellm-python</code>)</li> <li>Use feature flags to conditionally compile with/without PyO3</li> </ol> <p>This would allow both Rust and Python consumers, but is not currently implemented.</p>"},{"location":"guides/rust-usage/#summary","title":"Summary","text":"<ul> <li>\u2705 Use from Python: Yes, that's the primary use case</li> <li>\u274c Use directly from Rust: No, not currently possible</li> <li>\u26a0\ufe0f Embed Python in Rust: Yes, possible but adds complexity</li> <li>\u2705 Learn from the Rust code: Yes, you can read and understand the implementation</li> <li>\u2705 Reimplement in pure Rust: Yes, the code shows you how</li> </ul> <p>For most use cases, we recommend using RiteLLM from Python as designed, or creating a pure Rust implementation based on the patterns shown in this codebase.</p>"}]}